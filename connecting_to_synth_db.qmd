---
title: "Connecting to the Synth Health Database"
subtitle: "R and Python workflows for the `synth` schema"
---

```{r}
#| label: setup
#| include: false

# Locally on Windows, tell reticulate where Python is.
# In CI, RETICULATE_PYTHON env var is set by the workflow.
local_py <- "C:/Users/corey/synth_health_data/.conda/python.exe"
if (!nzchar(Sys.getenv("RETICULATE_PYTHON")) && file.exists(local_py)) {
  reticulate::use_python(local_py, required = TRUE)
}
```

## Why Connect Directly to the Database?

Health data analysis has traditionally followed a fragile pattern: an analyst
requests a data extract, someone runs a report or export, a CSV lands in an
email or shared folder, and the analyst opens it in a spreadsheet or statistical
software. This workflow is easy to describe and hard to defend. The extract is
a frozen snapshot with no provenance. Every subsequent analysis starts by
repeating the extract, often with subtly different filters or date ranges.
Reproducibility is an afterthought, and the gap between what is in the database
and what is on the analyst's desk grows with every revision.

Connecting directly to the database eliminates this gap. Instead of extracting
data and then analyzing it, the analyst works against the live source of truth.
Several important properties follow from this:

**Computation stays close to the data.** Databases are purpose-built for
aggregating and filtering large datasets. A `COUNT`, `GROUP BY`, or window
function that would strain a laptop's memory runs in milliseconds on a properly
indexed table. Direct connections let you push that work to where it belongs —
in the database — and only pull the summary you actually need into your
analysis environment.

**The workflow is reproducible by construction.** A script that connects to a
named database and runs named queries is self-documenting. Anyone with the same
credentials can re-run the exact analysis against the exact same data. There is
no intermediate file to lose, misplace, or accidentally overwrite.

**Access control and auditing are preserved.** Database connections respect
the permission model set by database administrators. Row-level security,
schema-level grants, and audit logs all remain intact. Exporting to a flat file
bypasses all of this.

**The interface is consistent across languages.** Whether you prefer R or
Python, the mental model is the same: establish a connection, send SQL, receive
a result. The packages shown in this document (`DBI`/`RPostgres` in R,
`SQLAlchemy`/`psycopg2` in Python) both provide a thin, idiomatic layer over
standard PostgreSQL wire protocol. You can switch between languages without
changing a line of SQL.

This document demonstrates this workflow against the `synth` schema — a
PostgreSQL database of synthetic Medicaid inpatient claims. The EDA steps shown
here are the same ones you would apply to any administrative claims database:
understand the schema, verify scale and shape, check data quality, and
characterize the key clinical and financial dimensions before writing a single
line of analysis code.

---

## Connection Details

| Parameter | Value      |
|-----------|------------|
| Host      | localhost  |
| Port      | 5433       |
| Database  | postgres   |
| Schema    | synth      |
| User      | postgres   |

The `fact_inpatient_stay` table is an analytic-ready fact table where each row
represents one resolved inpatient stay, assembled from the underlying messy
claims data. The remaining tables in the schema contain the raw messy claims,
deduplicated canonical versions, and reference tables used to build it.

---

## Prerequisites

Install the required packages before connecting.

::: {.panel-tabset}

## R

```r
install.packages(c("DBI", "RPostgres", "dplyr", "dbplyr"))
```

The key packages:

- **DBI** — a unified database interface that decouples your code from any
  specific database engine. You write the same `dbConnect()`, `dbGetQuery()`,
  and `dbDisconnect()` calls regardless of whether the back end is PostgreSQL,
  SQLite, or something else.
- **RPostgres** — the PostgreSQL-specific driver that DBI calls under the hood.
  It handles the network protocol, type conversion, and authentication.
- **dplyr** / **dbplyr** — together these let you write tidy `filter()`,
  `select()`, and `summarise()` calls that are automatically translated into
  SQL and executed in the database. The result is only pulled into R when you
  call `collect()`.

## Python

```bash
pip install sqlalchemy psycopg2-binary pandas
```

The key packages:

- **SQLAlchemy** — a database abstraction toolkit that manages connections,
  connection pooling, and dialect differences across database engines. Using an
  `engine` object rather than a raw connection makes your code portable and
  handles reconnection automatically.
- **psycopg2** — the PostgreSQL driver (the `-binary` variant bundles its own
  `libpq`, so no separate Postgres client install is needed on most systems).
- **pandas** — `pd.read_sql()` accepts a SQL string and an open connection and
  returns a DataFrame. It handles type inference, date parsing, and `NULL`-to-
  `NaN` conversion automatically.

:::

---

## Establishing a Connection

The connection object (R) or engine (Python) is the entry point for everything
that follows. Both accept the same five parameters: host, port, database name,
username, and password. A confirmation query that returns the server version is
a good sanity check before doing any real work — it verifies that the
credentials are correct and the network path is open.

::: {.panel-tabset}

## R

```{r}
#| label: r-connect

library(DBI)
library(RPostgres)
library(dplyr)
library(dbplyr)

con <- dbConnect(
  RPostgres::Postgres(),
  host     = "localhost",
  port     = 5433,
  dbname   = "postgres",
  user     = "postgres",
  password = "postgres"
)

# Confirm the connection is live
dbGetQuery(con, "SELECT current_database(), current_user, version()")
```

> **Tip:** Store credentials in environment variables (`.Renviron`) rather than
> hard-coding them. Hard-coded passwords end up in version control and in shared
> documents. Environment variables stay on the machine that needs them.
>
> ```r
> con <- dbConnect(
>   RPostgres::Postgres(),
>   host     = Sys.getenv("PG_HOST", "localhost"),
>   port     = as.integer(Sys.getenv("PG_PORT", 5433)),
>   dbname   = Sys.getenv("PG_DB",   "postgres"),
>   user     = Sys.getenv("PG_USER", "postgres"),
>   password = Sys.getenv("PG_PASSWORD")
> )
> ```

## Python

```{python}
#| label: py-connect

from sqlalchemy import create_engine, text
import pandas as pd

engine = create_engine(
    "postgresql+psycopg2://postgres:postgres@localhost:5433/postgres"
)

# Confirm the connection is live
with engine.connect() as conn:
    result = conn.execute(
        text("SELECT current_database(), current_user, version()")
    )
    print(result.fetchone())
```

> **Tip:** Store credentials in environment variables rather than the
> connection string. The `with engine.connect()` pattern also ensures the
> connection is returned to the pool when the block exits, even if an exception
> is raised.
>
> ```python
> import os
> from sqlalchemy import create_engine
>
> engine = create_engine(
>     f"postgresql+psycopg2://{os.environ['PG_USER']}:"
>     f"{os.environ['PG_PASSWORD']}@"
>     f"{os.environ.get('PG_HOST', 'localhost')}:"
>     f"{os.environ.get('PG_PORT', 5433)}/"
>     f"{os.environ.get('PG_DB', 'postgres')}"
> )
> ```

:::

---

## Exploring the Schema

Before touching any individual table, it is worth understanding what the schema
contains. PostgreSQL's `information_schema` views expose the catalog of tables,
columns, data types, and constraints without requiring any knowledge of the
underlying data. This is the database equivalent of reading a codebook — it
tells you what exists and how it is typed before you write a single analytical
query.

### Tables in the `synth` schema

The `information_schema.tables` view lists every table and view visible to the
current user. Checking this first confirms you are connected to the right
schema and have the access you expect. In a production environment this step
also surfaces any new tables added since the last time you worked with the
database.

::: {.panel-tabset}

## R

```{r}
#| label: r-list-tables

dbGetQuery(con, "
  SELECT table_name, table_type
  FROM   information_schema.tables
  WHERE  table_schema = 'synth'
  ORDER  BY table_name
")
```

## Python

```{python}
#| label: py-list-tables

with engine.connect() as conn:
    tables = pd.read_sql("""
        SELECT table_name, table_type
        FROM   information_schema.tables
        WHERE  table_schema = 'synth'
        ORDER  BY table_name
    """, conn)

print(tables.to_string(index=False))
```

:::

### Column definitions for `fact_inpatient_stay`

Reviewing column names, data types, and nullability before writing any queries
prevents a class of common errors: joining on a text column you assumed was
numeric, filtering a date stored as `character varying`, or expecting a NOT NULL
guarantee that does not exist. In claims data this step is especially important
because the same concept (a date, an amount, a code) can be stored as very
different types depending on the source system and the ETL that produced the
table.

::: {.panel-tabset}

## R

```{r}
#| label: r-columns

dbGetQuery(con, "
  SELECT column_name,
         data_type,
         is_nullable
  FROM   information_schema.columns
  WHERE  table_schema = 'synth'
    AND  table_name   = 'fact_inpatient_stay'
  ORDER  BY ordinal_position
")
```

## Python

```{python}
#| label: py-columns

with engine.connect() as conn:
    cols = pd.read_sql("""
        SELECT column_name,
               data_type,
               is_nullable
        FROM   information_schema.columns
        WHERE  table_schema = 'synth'
          AND  table_name   = 'fact_inpatient_stay'
        ORDER  BY ordinal_position
    """, conn)

print(cols.to_string(index=False))
```

:::

---

## Basic EDA: `fact_inpatient_stay`

Exploratory data analysis on a claims table follows a consistent sequence: start
with scale (how many rows?), then shape (what do the values look like?), then
quality (what is missing or anomalous?), and finally domain-specific dimensions
(time, geography, clinical codes, financials). Each step narrows the scope of
what you need to investigate further.

### Row count and shape

The very first thing to establish is the size of the table. This sets
expectations for query runtime, memory requirements if you plan to pull data
locally, and whether the row count matches what you were told to expect. A table
with 50 rows when you expected 50,000 is a sign of a failed load; a table with
5 million rows when you expected 500,000 means your analysis plan needs to
account for scale. Neither discovery should be made for the first time in the
middle of a slow `collect()`.

::: {.panel-tabset}

## R

```{r}
#| label: r-row-count

dbGetQuery(con, "SELECT COUNT(*) AS n_rows FROM synth.fact_inpatient_stay")
```

## Python

```{python}
#| label: py-row-count

with engine.connect() as conn:
    n = pd.read_sql(
        "SELECT COUNT(*) AS n_rows FROM synth.fact_inpatient_stay", conn
    )
print(n)
```

:::

### Sample rows

Looking at a small number of raw rows is the fastest way to spot problems that
summary statistics would hide: truncated identifiers, unexpected date formats,
codes that are all the same value (suggesting a default was applied everywhere),
or amounts with implausible precision. In claims data it is common to find
admission dates that are later than discharge dates, zero-dollar allowed amounts
on non-denied claims, or stay IDs that do not follow the expected pattern. A
5-row sample will not catch everything, but it will catch the obvious ones.

::: {.panel-tabset}

## R

```{r}
#| label: r-head

dbGetQuery(con, "SELECT * FROM synth.fact_inpatient_stay LIMIT 5")
```

## Python

```{python}
#| label: py-head

with engine.connect() as conn:
    sample = pd.read_sql(
        "SELECT * FROM synth.fact_inpatient_stay LIMIT 5", conn
    )
print(sample.to_string(index=False))
```

:::

### Summary statistics for numeric columns

Summary statistics computed in the database are far more efficient than pulling
all rows and computing them locally — the database returns a single row no
matter how large the table is. For inpatient claims, the key numeric fields are
length of stay, ICU utilization, and financial amounts (paid vs. allowed).
The gap between `total_allowed` and `total_paid` reflects the denied portion of
claims and is a primary driver of provider payment variability. The spread of
`length_of_stay` and `icu_days` captures clinical intensity. Implausibly large
maximums (e.g., a stay of 3,000 days) or negative values for amounts are
immediate data quality flags.

::: {.panel-tabset}

## R

```{r}
#| label: r-summary

dbGetQuery(con, "
  SELECT
    ROUND(MIN(length_of_stay)::numeric, 1)  AS los_min,
    ROUND(AVG(length_of_stay)::numeric, 1)  AS los_mean,
    ROUND(MAX(length_of_stay)::numeric, 1)  AS los_max,
    ROUND(MIN(icu_days)::numeric, 1)        AS icu_min,
    ROUND(AVG(icu_days)::numeric, 1)        AS icu_mean,
    ROUND(MAX(icu_days)::numeric, 1)        AS icu_max,
    ROUND(MIN(total_paid)::numeric, 2)      AS paid_min,
    ROUND(AVG(total_paid)::numeric, 2)      AS paid_mean,
    ROUND(MAX(total_paid)::numeric, 2)      AS paid_max,
    ROUND(MIN(total_allowed)::numeric, 2)   AS allowed_min,
    ROUND(AVG(total_allowed)::numeric, 2)   AS allowed_mean,
    ROUND(MAX(total_allowed)::numeric, 2)   AS allowed_max
  FROM synth.fact_inpatient_stay
")
```

When you do need full distributional detail, `dbplyr` lets you select only the
columns you need and pull them into local memory with `collect()`. This keeps
the transferred data minimal while giving you access to R's full suite of
summary and visualization tools.

```{r}
#| label: r-summary-local

fact <- con |>
  tbl(in_schema("synth", "fact_inpatient_stay")) |>
  select(length_of_stay, icu_days, total_paid, total_allowed,
         n_claims, any_denied) |>
  collect()

summary(fact)
```

## Python

```{python}
#| label: py-summary

with engine.connect() as conn:
    fact = pd.read_sql(
        """
        SELECT length_of_stay, icu_days, total_paid, total_allowed,
               n_claims, any_denied
        FROM   synth.fact_inpatient_stay
        """,
        conn
    )

print(fact.describe().round(2).to_string())
```

:::

### Missing values

In administrative claims, missingness is rarely random. A null `drg_cd` often
indicates a claim type that is not subject to DRG grouping (e.g., a psychiatric
or rehabilitation stay) or a grouper failure. A null `dx_cd_1` may reflect a
data submission gap from a specific provider or state. Financial nulls on
`total_paid` or `total_allowed` can indicate the stay was fully denied or that
payment information was not transmitted with the claim. Understanding which
fields are missing — and how many — is necessary before deciding how to handle
them in analysis. Computing missingness in the database avoids the cost of
pulling every row locally to run `.isna()` or `is.na()`.

::: {.panel-tabset}

## R

```{r}
#| label: r-nulls

dbGetQuery(con, "
  SELECT
    COUNT(*) FILTER (WHERE drg_cd       IS NULL) AS drg_cd_null,
    COUNT(*) FILTER (WHERE dx_cd_1      IS NULL) AS dx_cd_1_null,
    COUNT(*) FILTER (WHERE total_paid   IS NULL) AS total_paid_null,
    COUNT(*) FILTER (WHERE total_allowed IS NULL) AS total_allowed_null,
    COUNT(*)                                     AS n_total
  FROM synth.fact_inpatient_stay
")
```

## Python

```{python}
#| label: py-nulls

with engine.connect() as conn:
    nulls = pd.read_sql("""
        SELECT
          COUNT(*) FILTER (WHERE drg_cd       IS NULL) AS drg_cd_null,
          COUNT(*) FILTER (WHERE dx_cd_1      IS NULL) AS dx_cd_1_null,
          COUNT(*) FILTER (WHERE total_paid   IS NULL) AS total_paid_null,
          COUNT(*) FILTER (WHERE total_allowed IS NULL) AS total_allowed_null,
          COUNT(*)                                     AS n_total
        FROM synth.fact_inpatient_stay
    """, conn)

print(nulls.to_string(index=False))
```

:::

### Stays by admission year and quarter

Temporal distribution is one of the first things to examine in longitudinal
claims data. You want to confirm that volume is stable and consistent with
expectations across the observation window. Large drops in a particular quarter
can signal incomplete data submission — states or providers that were late in
submitting claims for that period. Unexpected spikes may indicate a system
change that altered how stays were constructed. In Medicaid data specifically,
enrollment fluctuations (eligibility changes, policy transitions) can drive
genuine quarter-over-quarter volume changes that are worth distinguishing from
data artifacts.

::: {.panel-tabset}

## R

```{r}
#| label: r-year-qtr

con |>
  tbl(in_schema("synth", "fact_inpatient_stay")) |>
  count(admit_year, admit_quarter) |>
  arrange(admit_year, admit_quarter) |>
  collect()
```

## Python

```{python}
#| label: py-year-qtr

with engine.connect() as conn:
    trend = pd.read_sql("""
        SELECT admit_year,
               admit_quarter,
               COUNT(*) AS n_stays
        FROM   synth.fact_inpatient_stay
        GROUP  BY admit_year, admit_quarter
        ORDER  BY admit_year, admit_quarter
    """, conn)

print(trend.to_string(index=False))
```

:::

### Stays by state

Even in a single-state dataset it is important to verify that the geographic
distribution matches expectations. In multi-state Medicaid data, volume by state
is one of the first quality checks performed — states with very low counts
relative to their enrolled population may have submission problems, while states
with inflated counts may have duplicate claim issues that survived the
deduplication step. The `state_cd` and `submtg_state_cd` fields in this schema
represent the state where the beneficiary was enrolled and the state that
submitted the claim, respectively. In most cases these match, but discrepancies
can surface cross-state coverage situations.

::: {.panel-tabset}

## R

```{r}
#| label: r-state

con |>
  tbl(in_schema("synth", "fact_inpatient_stay")) |>
  count(state_cd, sort = TRUE) |>
  collect()
```

## Python

```{python}
#| label: py-state

with engine.connect() as conn:
    by_state = pd.read_sql("""
        SELECT state_cd,
               COUNT(*) AS n_stays
        FROM   synth.fact_inpatient_stay
        GROUP  BY state_cd
        ORDER  BY n_stays DESC
    """, conn)

print(by_state.to_string(index=False))
```

:::

### Denial rate

The `any_denied` flag indicates whether at least one claim contributing to the
stay was denied. Denial rates are a key utilization management metric: they
affect provider payment, they can signal where prior authorization requirements
are concentrated, and they are often stratified by DRG or diagnosis in
downstream analysis. A denial rate of near zero suggests either a clean payer-
provider relationship or that denial information was not captured in the source
data. A very high rate (above 20–25% in most Medicaid contexts) may indicate
data quality problems with the denied claim flag itself. The rate here reflects
the simulation parameters and should be consistent with the ~5% denied-claim
probability used to generate the data.

::: {.panel-tabset}

## R

```{r}
#| label: r-denial

dbGetQuery(con, "
  SELECT
    SUM(any_denied)                                    AS n_any_denied,
    COUNT(*)                                           AS n_total,
    ROUND(AVG(any_denied::numeric) * 100, 1) || '%'   AS denial_rate
  FROM synth.fact_inpatient_stay
")
```

## Python

```{python}
#| label: py-denial

with engine.connect() as conn:
    denial = pd.read_sql("""
        SELECT
          SUM(any_denied)                          AS n_any_denied,
          COUNT(*)                                 AS n_total,
          ROUND(AVG(any_denied::numeric) * 100, 1) AS denial_rate_pct
        FROM synth.fact_inpatient_stay
    """, conn)

print(denial.to_string(index=False))
```

:::

---

## Closing the Connection

Always close the connection explicitly when you are done. In R, an open `con`
object holds a network socket; in Python, `engine.dispose()` closes the
underlying connection pool. Leaving connections open is not catastrophic in a
single-user local environment, but in shared or production settings it consumes
database resources and can hit connection limits.

::: {.panel-tabset}

## R

```{r}
#| label: r-disconnect

DBI::dbDisconnect(con)
```

## Python

```{python}
#| label: py-disconnect

engine.dispose()
```

:::
